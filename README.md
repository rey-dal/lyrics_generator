-General Steps-

1-Predict the next word in the sequence

2-Feed old input plus a new output word as the next input

3-Make the output a one-hot encoded label over entire corpus of desired text outputs

4-Utilize categorical cross entropy loss function as the output will be multi-class

Model Architecture: Bi-LSTM, Embedding

Epochs: 200

Input = "i am just a model"

Output = "i am just a model little bit more just a little bit more just a club secret question watch watch depressed...ahhaha escape drums paris"

I mean, bro is vibin' 
